<extensionManifest><groupId>org.apache.nifi</groupId><artifactId>nifi-kafka-nar</artifactId><version>2.4.0</version><parentNar><groupId>org.apache.nifi</groupId><artifactId>nifi-kafka-service-api-nar</artifactId><version>2.4.0</version></parentNar><systemApiVersion>2.1.0</systemApiVersion><buildInfo><tag>HEAD</tag><branch>UNKNOWN</branch><revision>65c7732</revision></buildInfo><extensions><extension><name>org.apache.nifi.kafka.processors.ConsumeKafka</name><type>PROCESSOR</type><description>Consumes messages from Apache Kafka Consumer API. The complementary NiFi processor for sending messages is PublishKafka. The Processor supports consumption of Kafka messages, optionally interpreted as NiFi records. Please note that, at this time (in read record mode), the Processor assumes that all records that are retrieved from a given partition have the same schema. For this mode, if any of the Kafka messages are pulled but cannot be parsed or written with the configured Record Reader or Record Writer, the contents of the message will be written to a separate FlowFile, and that FlowFile will be transferred to the 'parse.failure' relationship. Otherwise, each FlowFile is sent to the 'success' relationship and may contain many individual messages within the single FlowFile. A 'record.count' attribute is added to indicate how many messages are contained in the FlowFile. No two Kafka messages will be placed into the same FlowFile if they have different schemas, or if they have different values for a message header that is included by the &lt;Headers to Add as Attributes&gt; property.</description><tags><tag>Kafka</tag><tag>Get</tag><tag>Record</tag><tag>csv</tag><tag>avro</tag><tag>json</tag><tag>Ingest</tag><tag>Ingress</tag><tag>Topic</tag><tag>PubSub</tag><tag>Consume</tag></tags><properties><property><name>Kafka Connection Service</name><displayName>Kafka Connection Service</displayName><description>Provides connections to Kafka Broker for publishing Kafka Records</description><controllerServiceDefinition><className>org.apache.nifi.kafka.service.api.KafkaConnectionService</className><groupId>org.apache.nifi</groupId><artifactId>nifi-kafka-service-api-nar</artifactId><version>2.4.0</version></controllerServiceDefinition><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Group ID</name><displayName>Group ID</displayName><description>Kafka Consumer Group Identifier corresponding to Kafka group.id property</description><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Topic Format</name><displayName>Topic Format</displayName><description>Specifies whether the Topics provided are a comma separated list of names or a single regular expression</description><defaultValue>names</defaultValue><allowableValues><allowableValue><displayName>names</displayName><value>names</value><description>Topic is a full topic name or comma separated list of names</description></allowableValue><allowableValue><displayName>pattern</displayName><value>pattern</value><description>Topic is a regular expression according to the Java Pattern syntax</description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Topics</name><displayName>Topics</displayName><description>The name or pattern of the Kafka Topics from which the Processor consumes Kafka Records. More than one can be supplied if comma separated.</description><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>ENVIRONMENT</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>auto.offset.reset</name><displayName>Auto Offset Reset</displayName><description>Automatic offset configuration applied when no previous consumer offset found corresponding to Kafka auto.offset.reset property</description><defaultValue>latest</defaultValue><allowableValues><allowableValue><displayName>earliest</displayName><value>earliest</value><description>Automatically reset the offset to the earliest offset</description></allowableValue><allowableValue><displayName>latest</displayName><value>latest</value><description>Automatically reset the offset to the latest offset</description></allowableValue><allowableValue><displayName>none</displayName><value>none</value><description>Throw exception to the consumer if no previous offset found for the consumer group</description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Commit Offsets</name><displayName>Commit Offsets</displayName><description>Specifies whether this Processor should commit the offsets to Kafka after receiving messages. Typically, this value should be set to true so that messages that are received are not duplicated. However, in certain scenarios, we may want to avoid committing the offsets, that the data can be processed and later acknowledged by PublishKafka in order to provide Exactly Once semantics.</description><defaultValue>true</defaultValue><allowableValues><allowableValue><displayName>true</displayName><value>true</value><description></description></allowableValue><allowableValue><displayName>false</displayName><value>false</value><description></description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Max Uncommitted Time</name><displayName>Max Uncommitted Time</displayName><description>Specifies the maximum amount of time that the Processor can consume from Kafka before it must transfer FlowFiles on through the flow and commit the offsets to Kafka (if appropriate). A larger time period can result in longer latency.</description><defaultValue>100 millis</defaultValue><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Header Name Pattern</name><displayName>Header Name Pattern</displayName><description>Regular Expression Pattern applied to Kafka Record Header Names for selecting Header Values to be written as FlowFile attributes</description><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Header Encoding</name><displayName>Header Encoding</displayName><description>Character encoding applied when reading Kafka Record Header values and writing FlowFile attributes</description><defaultValue>UTF-8</defaultValue><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Processing Strategy</name><displayName>Processing Strategy</displayName><description>Strategy for processing Kafka Records and writing serialized output to FlowFiles</description><defaultValue>FLOW_FILE</defaultValue><allowableValues><allowableValue><displayName>FLOW_FILE</displayName><value>FLOW_FILE</value><description>Write one FlowFile for each Kafka Record consumed</description></allowableValue><allowableValue><displayName>DEMARCATOR</displayName><value>DEMARCATOR</value><description>Write one FlowFile for each batch of Kafka Records consumed (optionally grouped by Kafka key)</description></allowableValue><allowableValue><displayName>RECORD</displayName><value>RECORD</value><description>Write one FlowFile containing multiple Kafka Records consumed and processed with Record Reader and Record Writer</description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Record Reader</name><displayName>Record Reader</displayName><description>The Record Reader to use for incoming Kafka messages</description><controllerServiceDefinition><className>org.apache.nifi.serialization.RecordReaderFactory</className><groupId>org.apache.nifi</groupId><artifactId>nifi-standard-services-api-nar</artifactId><version>2.4.0</version></controllerServiceDefinition><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic><dependencies><dependency><propertyName>Processing Strategy</propertyName><propertyDisplayName>Processing Strategy</propertyDisplayName><dependentValues><dependentValue>RECORD</dependentValue></dependentValues></dependency></dependencies></property><property><name>Record Writer</name><displayName>Record Writer</displayName><description>The Record Writer to use in order to serialize the outgoing FlowFiles</description><controllerServiceDefinition><className>org.apache.nifi.serialization.RecordSetWriterFactory</className><groupId>org.apache.nifi</groupId><artifactId>nifi-standard-services-api-nar</artifactId><version>2.4.0</version></controllerServiceDefinition><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic><dependencies><dependency><propertyName>Processing Strategy</propertyName><propertyDisplayName>Processing Strategy</propertyDisplayName><dependentValues><dependentValue>RECORD</dependentValue></dependentValues></dependency></dependencies></property><property><name>Output Strategy</name><displayName>Output Strategy</displayName><description>The format used to output the Kafka Record into a FlowFile Record.</description><defaultValue>USE_VALUE</defaultValue><allowableValues><allowableValue><displayName>Use Content as Value</displayName><value>USE_VALUE</value><description>Write only the Kafka Record value to the FlowFile record.</description></allowableValue><allowableValue><displayName>Use Wrapper</displayName><value>USE_WRAPPER</value><description>Write the Kafka Record key, value, headers, and metadata into the FlowFile record. (See processor usage for more information.)</description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic><dependencies><dependency><propertyName>Processing Strategy</propertyName><propertyDisplayName>Processing Strategy</propertyDisplayName><dependentValues><dependentValue>RECORD</dependentValue></dependentValues></dependency></dependencies></property><property><name>Key Attribute Encoding</name><displayName>Key Attribute Encoding</displayName><description>Encoding for value of configured FlowFile attribute containing Kafka Record Key.</description><defaultValue>utf-8</defaultValue><allowableValues><allowableValue><displayName>UTF-8 Encoded</displayName><value>utf-8</value><description>The key is interpreted as a UTF-8 Encoded string.</description></allowableValue><allowableValue><displayName>Hex Encoded</displayName><value>hex</value><description>The key is interpreted as arbitrary binary data and is encoded using hexadecimal characters with uppercase letters</description></allowableValue><allowableValue><displayName>Do Not Add Key as Attribute</displayName><value>do-not-add</value><description>The key will not be added as an Attribute</description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Key Format</name><displayName>Key Format</displayName><description>Specifies how to represent the Kafka Record Key in the output FlowFile</description><defaultValue>byte-array</defaultValue><allowableValues><allowableValue><displayName>String</displayName><value>string</value><description>Format the Kafka ConsumerRecord key as a UTF-8 string.</description></allowableValue><allowableValue><displayName>Byte Array</displayName><value>byte-array</value><description>Format the Kafka ConsumerRecord key as a byte array.</description></allowableValue><allowableValue><displayName>Record</displayName><value>record</value><description>Format the Kafka ConsumerRecord key as a record.</description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic><dependencies><dependency><propertyName>Output Strategy</propertyName><propertyDisplayName>Output Strategy</propertyDisplayName><dependentValues><dependentValue>USE_WRAPPER</dependentValue></dependentValues></dependency></dependencies></property><property><name>Key Record Reader</name><displayName>Key Record Reader</displayName><description>The Record Reader to use for parsing the Kafka Record Key into a Record</description><controllerServiceDefinition><className>org.apache.nifi.serialization.RecordReaderFactory</className><groupId>org.apache.nifi</groupId><artifactId>nifi-standard-services-api-nar</artifactId><version>2.4.0</version></controllerServiceDefinition><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic><dependencies><dependency><propertyName>Key Format</propertyName><propertyDisplayName>Key Format</propertyDisplayName><dependentValues><dependentValue>record</dependentValue></dependentValues></dependency></dependencies></property><property><name>Message Demarcator</name><displayName>Message Demarcator</displayName><description>Since KafkaConsumer receives messages in batches, this Processor has an option to output FlowFiles which contains all Kafka messages in a single batch for a given topic and partition and this property allows you to provide a string (interpreted as UTF-8) to use for demarcating apart multiple Kafka messages. This is an optional property and if not provided each Kafka message received will result in a single FlowFile which  time it is triggered. To enter special character such as 'new line' use CTRL+Enter or Shift+Enter depending on the OS</description><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic><dependencies><dependency><propertyName>Processing Strategy</propertyName><propertyDisplayName>Processing Strategy</propertyDisplayName><dependentValues><dependentValue>DEMARCATOR</dependentValue></dependentValues></dependency></dependencies></property><property><name>Separate By Key</name><displayName>Separate By Key</displayName><description>When this property is enabled, two messages will only be added to the same FlowFile if both of the Kafka Messages have identical keys.</description><defaultValue>false</defaultValue><allowableValues><allowableValue><displayName>true</displayName><value>true</value><description></description></allowableValue><allowableValue><displayName>false</displayName><value>false</value><description></description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic><dependencies><dependency><propertyName>Message Demarcator</propertyName><propertyDisplayName>Message Demarcator</propertyDisplayName></dependency></dependencies></property></properties><relationships><relationship><name>success</name><description>FlowFiles containing one or more serialized Kafka Records</description><autoTerminated>false</autoTerminated></relationship></relationships><writesAttributes><writesAttribute><name>record.count</name><description>The number of records received</description></writesAttribute><writesAttribute><name>mime.type</name><description>The MIME Type that is provided by the configured Record Writer</description></writesAttribute><writesAttribute><name>kafka.count</name><description>The number of messages written if more than one</description></writesAttribute><writesAttribute><name>kafka.key</name><description>The key of message if present and if single message. How the key is encoded depends on the value of the 'Key Attribute Encoding' property.</description></writesAttribute><writesAttribute><name>kafka.offset</name><description>The offset of the message in the partition of the topic.</description></writesAttribute><writesAttribute><name>kafka.timestamp</name><description>The timestamp of the message in the partition of the topic.</description></writesAttribute><writesAttribute><name>kafka.partition</name><description>The partition of the topic the message or message bundle is from</description></writesAttribute><writesAttribute><name>kafka.topic</name><description>The topic the message or message bundle is from</description></writesAttribute><writesAttribute><name>kafka.tombstone</name><description>Set to true if the consumed message is a tombstone message</description></writesAttribute></writesAttributes><inputRequirement>INPUT_FORBIDDEN</inputRequirement><seeAlso><see>org.apache.nifi.kafka.processors.PublishKafka</see></seeAlso></extension><extension><name>org.apache.nifi.kafka.processors.PublishKafka</name><type>PROCESSOR</type><description>Sends the contents of a FlowFile as either a message or as individual records to Apache Kafka using the Kafka Producer API. The messages to send may be individual FlowFiles, may be delimited using a user-specified delimiter (such as a new-line), or may be record-oriented data that can be read by the configured Record Reader. The complementary NiFi processor for fetching messages is ConsumeKafka. To produce a kafka tombstone message while using PublishStrategy.USE_WRAPPER, simply set the value of a record to 'null'.</description><tags><tag>Apache</tag><tag>Kafka</tag><tag>Record</tag><tag>csv</tag><tag>json</tag><tag>avro</tag><tag>logs</tag><tag>Put</tag><tag>Send</tag><tag>Message</tag><tag>PubSub</tag></tags><properties><property><name>Kafka Connection Service</name><displayName>Kafka Connection Service</displayName><description>Provides connections to Kafka Broker for publishing Kafka Records</description><controllerServiceDefinition><className>org.apache.nifi.kafka.service.api.KafkaConnectionService</className><groupId>org.apache.nifi</groupId><artifactId>nifi-kafka-service-api-nar</artifactId><version>2.4.0</version></controllerServiceDefinition><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Topic Name</name><displayName>Topic Name</displayName><description>Name of the Kafka Topic to which the Processor publishes Kafka Records</description><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Failure Strategy</name><displayName>Failure Strategy</displayName><description>Specifies how the processor handles a FlowFile if it is unable to publish the data to Kafka</description><defaultValue>Route to Failure</defaultValue><allowableValues><allowableValue><displayName>Route to Failure</displayName><value>Route to Failure</value><description>When unable to publish records to Kafka, the FlowFile will be routed to the failure relationship.</description></allowableValue><allowableValue><displayName>Rollback</displayName><value>Rollback</value><description>When unable to publish records to Kafka, the FlowFile will be placed back on the queue so that it will be retried. For flows where FlowFile ordering is important, this strategy can be used along with ensuring that the each processor uses only a single Concurrent Task.</description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>acks</name><displayName>Delivery Guarantee</displayName><description>Specifies the requirement for guaranteeing that a message is sent to Kafka. Corresponds to Kafka Client acks property.</description><defaultValue>all</defaultValue><allowableValues><allowableValue><displayName>Guarantee Replicated Delivery</displayName><value>all</value><description>FlowFile will be routed to failure unless the message is replicated to the appropriate number of Kafka Nodes according to the Topic configuration</description></allowableValue><allowableValue><displayName>Guarantee Single Node Delivery</displayName><value>1</value><description>FlowFile will be routed to success if the message is received by a single Kafka node, whether or not it is replicated. This is faster than &lt;Guarantee Replicated Delivery&gt; but can result in data loss if a Kafka node crashes</description></allowableValue><allowableValue><displayName>Best Effort</displayName><value>0</value><description>FlowFile will be routed to success after successfully sending the content to a Kafka node, without waiting for any acknowledgment from the node at all. This provides the best performance but may result in data loss.</description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>compression.type</name><displayName>Compression Type</displayName><description>Specifies the compression strategy for records sent to Kafka. Corresponds to Kafka Client compression.type property.</description><defaultValue>none</defaultValue><allowableValues><allowableValue><displayName>none</displayName><value>none</value><description></description></allowableValue><allowableValue><displayName>gzip</displayName><value>gzip</value><description></description></allowableValue><allowableValue><displayName>snappy</displayName><value>snappy</value><description></description></allowableValue><allowableValue><displayName>lz4</displayName><value>lz4</value><description></description></allowableValue><allowableValue><displayName>zstd</displayName><value>zstd</value><description></description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>max.request.size</name><displayName>Max Request Size</displayName><description>The maximum size of a request in bytes. Corresponds to Kafka Client max.request.size property.</description><defaultValue>1 MB</defaultValue><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Transactions Enabled</name><displayName>Transactions Enabled</displayName><description>Specifies whether to provide transactional guarantees when communicating with Kafka. If there is a problem sending data to Kafka, and this property is set to false, then the messages that have already been sent to Kafka will continue on and be delivered to consumers. If this is set to true, then the Kafka transaction will be rolled back so that those messages are not available to consumers. Setting this to true requires that the [Delivery Guarantee] property be set to [Guarantee Replicated Delivery.]</description><defaultValue>true</defaultValue><allowableValues><allowableValue><displayName>true</displayName><value>true</value><description></description></allowableValue><allowableValue><displayName>false</displayName><value>false</value><description></description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Transactional ID Prefix</name><displayName>Transactional ID Prefix</displayName><description>Specifies the KafkaProducer config transactional.id will be a generated UUID and will be prefixed with the configured string.</description><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic><dependencies><dependency><propertyName>Transactions Enabled</propertyName><propertyDisplayName>Transactions Enabled</propertyDisplayName><dependentValues><dependentValue>true</dependentValue></dependentValues></dependency></dependencies></property><property><name>partitioner.class</name><displayName>Partitioner Class</displayName><description>Specifies which class to use to compute a partition id for a message. Corresponds to Kafka Client partitioner.class property.</description><defaultValue>org.apache.kafka.clients.producer.internals.DefaultPartitioner</defaultValue><allowableValues><allowableValue><displayName>RoundRobinPartitioner</displayName><value>org.apache.kafka.clients.producer.RoundRobinPartitioner</value><description>Messages will be assigned partitions in a round-robin fashion, sending the first message to Partition 1, the next Partition to Partition 2, and so on, wrapping as necessary.</description></allowableValue><allowableValue><displayName>DefaultPartitioner</displayName><value>org.apache.kafka.clients.producer.internals.DefaultPartitioner</value><description>The default partitioning strategy will choose the Strictly Uniform Sticky Partitioner (See KIP-794 for more details and available configuration options that can be used via dynamic properties).</description></allowableValue><allowableValue><displayName>Expression Language Partitioner</displayName><value>org.apache.nifi.processors.kafka.pubsub.Partitioners.ExpressionLanguagePartitioner</value><description>Interprets the &lt;Partition&gt; property as Expression Language that will be evaluated against each FlowFile. This Expression will be evaluated once against the FlowFile, so all Records in a given FlowFile will go to the same partition.</description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>partition</name><displayName>Partition</displayName><description>Specifies the Kafka Partition destination for Records.</description><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Message Demarcator</name><displayName>Message Demarcator</displayName><description>Specifies the string (interpreted as UTF-8) to use for demarcating multiple messages within a single FlowFile. If not specified, the entire content of the FlowFile will be used as a single message. If specified, the contents of the FlowFile will be split on this delimiter and each section sent as a separate Kafka message. To enter special character such as 'new line' use CTRL+Enter or Shift+Enter, depending on your OS.</description><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Record Reader</name><displayName>Record Reader</displayName><description>The Record Reader to use for incoming FlowFiles</description><controllerServiceDefinition><className>org.apache.nifi.serialization.RecordReaderFactory</className><groupId>org.apache.nifi</groupId><artifactId>nifi-standard-services-api-nar</artifactId><version>2.4.0</version></controllerServiceDefinition><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Record Writer</name><displayName>Record Writer</displayName><description>The Record Writer to use in order to serialize the data before sending to Kafka</description><controllerServiceDefinition><className>org.apache.nifi.serialization.RecordSetWriterFactory</className><groupId>org.apache.nifi</groupId><artifactId>nifi-standard-services-api-nar</artifactId><version>2.4.0</version></controllerServiceDefinition><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Publish Strategy</name><displayName>Publish Strategy</displayName><description>The format used to publish the incoming FlowFile record to Kafka.</description><defaultValue>USE_VALUE</defaultValue><allowableValues><allowableValue><displayName>Use Content as Record Value</displayName><value>USE_VALUE</value><description>Write only the FlowFile content to the Kafka Record value.</description></allowableValue><allowableValue><displayName>Use Wrapper</displayName><value>USE_WRAPPER</value><description>Write the Kafka Record key, value, headers, and metadata into the Kafka Record value.  (See processor usage for more information.)</description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic><dependencies><dependency><propertyName>Record Reader</propertyName><propertyDisplayName>Record Reader</propertyDisplayName></dependency></dependencies></property><property><name>Message Key Field</name><displayName>Message Key Field</displayName><description>The name of a field in the Input Records that should be used as the Key for the Kafka message.</description><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic><dependencies><dependency><propertyName>Publish Strategy</propertyName><propertyDisplayName>Publish Strategy</propertyDisplayName><dependentValues><dependentValue>USE_VALUE</dependentValue></dependentValues></dependency></dependencies></property><property><name>FlowFile Attribute Header Pattern</name><displayName>FlowFile Attribute Header Pattern</displayName><description>A Regular Expression that is matched against all FlowFile attribute names. Any attribute whose name matches the pattern will be added to the Kafka messages as a Header. If not specified, no FlowFile attributes will be added as headers.</description><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Header Encoding</name><displayName>Header Encoding</displayName><description>For any attribute that is added as a Kafka Record Header, this property indicates the Character Encoding to use for serializing the headers.</description><defaultValue>UTF-8</defaultValue><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic><dependencies><dependency><propertyName>FlowFile Attribute Header Pattern</propertyName><propertyDisplayName>FlowFile Attribute Header Pattern</propertyDisplayName></dependency></dependencies></property><property><name>Kafka Key</name><displayName>Kafka Key</displayName><description>The Key to use for the Message. If not specified, the FlowFile attribute 'kafka.key' is used as the message key, if it is present.Beware that setting Kafka key and demarcating at the same time may potentially lead to many Kafka messages with the same key.Normally this is not a problem as Kafka does not enforce or assume message and key uniqueness. Still, setting the demarcator and Kafka key at the same time poses a risk of data loss on Kafka. During a topic compaction on Kafka, messages will be deduplicated based on this key.</description><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>true</expressionLanguageSupported><expressionLanguageScope>FLOWFILE_ATTRIBUTES</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Kafka Key Attribute Encoding</name><displayName>Kafka Key Attribute Encoding</displayName><description>FlowFiles that are emitted have an attribute named 'kafka.key'. This property dictates how the value of the attribute should be encoded.</description><defaultValue>utf-8</defaultValue><allowableValues><allowableValue><displayName>UTF-8 Encoded</displayName><value>utf-8</value><description>The key is interpreted as a UTF-8 Encoded string.</description></allowableValue><allowableValue><displayName>Hex Encoded</displayName><value>hex</value><description>The key is interpreted as arbitrary binary data and is encoded using hexadecimal characters with uppercase letters</description></allowableValue><allowableValue><displayName>Do Not Add Key as Attribute</displayName><value>do-not-add</value><description>The key will not be added as an Attribute</description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Record Key Writer</name><displayName>Record Key Writer</displayName><description>The Record Key Writer to use for outgoing FlowFiles</description><controllerServiceDefinition><className>org.apache.nifi.serialization.RecordSetWriterFactory</className><groupId>org.apache.nifi</groupId><artifactId>nifi-standard-services-api-nar</artifactId><version>2.4.0</version></controllerServiceDefinition><required>false</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic></property><property><name>Record Metadata Strategy</name><displayName>Record Metadata Strategy</displayName><description>Specifies whether the Record's metadata (topic and partition) should come from the Record's metadata field or if it should come from the configured Topic Name and Partition / Partitioner class properties</description><defaultValue>FROM_PROPERTIES</defaultValue><allowableValues><allowableValue><displayName>Metadata From Record</displayName><value>FROM_RECORD</value><description>The Kafka Record's Topic and Partition will be determined by looking at the /metadata/topic and /metadata/partition fields of the Record, respectively. If these fields are invalid or not present, the Topic Name and Partition/Partitioner class properties of the processor will be considered.</description></allowableValue><allowableValue><displayName>Use Configured Values</displayName><value>FROM_PROPERTIES</value><description>The Kafka Record's Topic will be determined using the 'Topic Name' processor property. The partition will be determined using the 'Partition' and 'Partitioner class' properties.</description></allowableValue></allowableValues><required>true</required><sensitive>false</sensitive><expressionLanguageSupported>false</expressionLanguageSupported><expressionLanguageScope>NONE</expressionLanguageScope><dynamicallyModifiesClasspath>false</dynamicallyModifiesClasspath><dynamic>false</dynamic><dependencies><dependency><propertyName>Publish Strategy</propertyName><propertyDisplayName>Publish Strategy</propertyDisplayName><dependentValues><dependentValue>USE_WRAPPER</dependentValue></dependentValues></dependency></dependencies></property></properties><relationships><relationship><name>failure</name><description>Any FlowFile that cannot be sent to Kafka will be routed to this Relationship</description><autoTerminated>false</autoTerminated></relationship><relationship><name>success</name><description>FlowFiles for which all content was sent to Kafka.</description><autoTerminated>false</autoTerminated></relationship></relationships><readsAttributes><readsAttribute><name>kafka.tombstone</name><description>If this attribute is set to 'true', if the processor is not configured with a demarcator and if the FlowFile's content is null, then a tombstone message with zero bytes will be sent to Kafka.</description></readsAttribute></readsAttributes><writesAttributes><writesAttribute><name>msg.count</name><description>The number of messages that were sent to Kafka for this FlowFile. This attribute is added only to FlowFiles that are routed to success.</description></writesAttribute></writesAttributes><inputRequirement>INPUT_REQUIRED</inputRequirement><seeAlso><see>org.apache.nifi.kafka.processors.ConsumeKafka</see></seeAlso></extension></extensions></extensionManifest>